# -*- coding: utf-8 -*-
"""submission_energy_consumption_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fjqxXuQ7e42vG7vPb6j_mSyyBc20DHAb

# Energy Consumption Prediction

## Problem Statement

Konsumsi energi pada bangunan sulit diprediksi secara akurat karena dipengaruhi oleh banyak faktor, seperti cuaca, waktu, dan kebiasaan penggunaan. Tidak adanya sistem prediktif menyebabkan operator bangunan kesulitan dalam mengelola beban puncak, yang berdampak pada efisiensi dan biaya operasional.

## Data Exploration & Preparation

### Import Libraries
"""

!pip install xgboost

from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""### Load Dataset

Dataset yang digunakan dalam proyek ini berjudul **"Energy Consumption Prediction"**, yang diambil dari platform Kaggle:
https://www.kaggle.com/datasets/mrsimple07/energy-consumption-prediction/data

Fitur:
- Temperature
- Humidity
- Square Footage
- Occupancy
- HVAC (Heating, Ventilation, and Air Conditioning) Usage
- Lighting Usage
- Renewable Energy
- Day Of Week
- Holiday
- Energy Consumption
"""

df = pd.read_csv('energy_consumption.csv')
df.head()

"""#### Number of rows and columns"""

# Mengecek jumlah baris dan kolom
print(df.shape)

"""#### Data type of each column"""

# Mengecek tipe data tiap kolom
print(df.dtypes)

"""#### Checking for missing values"""

# Mengecek nilai yang hilang
print(df.isnull().sum())

"""#### Initial descriptive statistics"""

# Mengecek statistik deskriptif awal
df.describe()

"""### Feature Distribution

Distribusi fitur numerik divisualisasikan menggunakan histogram untuk mengetahui pola penyebaran data.   Langkah ini bertujuan untuk mengidentifikasi apakah data berdistribusi normal, miring (skewed), atau memiliki nilai ekstrem.
"""

num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Visualisasi distribusi fitur
for col in num_cols:
    plt.figure(figsize=(5, 3))
    sns.histplot(data=df, x=col, kde=True, bins=30)
    plt.title(f'{col}')
    plt.tight_layout()
    plt.show()

"""### Feature Correlation Heatmap

Korelasi antar fitur numerik dianalisis menggunakan heatmap. Langkah ini bertujuan untuk memahami hubungan antar variabel dan mendeteksi fitur yang mungkin saling berkaitan kuat.
"""

# Visualisasi heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

"""### Outlier Detection

Outlier atau nilai ekstrem dideteksi menggunakan metode IQR (Interquartile Range). Langkah ini bertujuan untuk mengetahui apakah terdapat data yang menyimpang jauh dari mayoritas nilai lainnya.
"""

# Ambil fitur numerik
num_cols = ['Temperature', 'Humidity', 'SquareFootage', 'Occupancy', 'RenewableEnergy']

# Simpan hasil outlier
outlier_report = {}

for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    outlier_report[col] = len(outliers)

# Visualisasi boxplot
for col in num_cols:
    plt.figure(figsize=(5, 3))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot: {col}')
    plt.tight_layout()
    plt.show()

"""### Preprocessing

#### Feature Selection

Pada tahap ini, dilakukan pemisahan antara variabel input (fitur/X) dan variabel yang ingin diprediksi (target/y). Fitur terdiri dari variabel-variabel yang diasumsikan memiliki pengaruh terhadap target. Sementara itu, target adalah variabel yang akan diprediksi oleh model, yaitu EnergyConsumption.
"""

# Memisahkan fitur dan target
features = [
    'Temperature', 'Humidity', 'SquareFootage', 'Occupancy',
    'HVACUsage', 'LightingUsage', 'RenewableEnergy',
    'DayOfWeek', 'Holiday'
]

X = df[features]
y = df['EnergyConsumption']

"""#### Categorical Data Encoding & Numerical Data Scaling

Fitur numerik dilakukan standardisasi agar berada dalam skala yang sama. Sementara itu, fitur kategorikal diubah menggunakan one-hot encoding. Hal ini dilakukan agar fitur tersebut dapat diproses oleh algoritma machine learning.
"""

# Mendeteksi kolom numerikal dan kategorikal
num_cols = ['Temperature', 'Humidity', 'SquareFootage', 'Occupancy', 'RenewableEnergy']
cat_cols = ['HVACUsage', 'LightingUsage', 'DayOfWeek', 'Holiday']

# Preprocessor
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
])

"""#### Data Splitting

Data dibagi menjadi dua bagian: data latih (training) dan data uji (testing) dengan rasio 80:20. Pembagian ini bertujuan untuk mengevaluasi performa model secara adil terhadap data yang belum pernah dilihat sebelumnya.
"""

# Membagi data ke dalam data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Modeling

Beberapa algoritma yang digunakan dalam proyek ini, yaitu:

- **Ridge Regression**: regresi linear dengan regularisasi L2.
- **Lasso Regression**: regresi linear dengan regularisasi L1.
- **Random Forest Regressor**: algoritma ensambel berbasis Decision Tree yang cukup kuat terhadap outlier.
- **XGBoost Regressor**: algoritma Boosting yang sering digunakan karena akurasinya yang tinggi dan efisiensinya.
"""

# Model pipeline (default)
baseline_models = {
    "Ridge Regression": Pipeline([
        ("preprocessor", preprocessor),
        ("model", Ridge())
    ]),
    "Lasso Regression": Pipeline([
        ("preprocessor", preprocessor),
        ("model", Lasso())
    ]),
    "Random Forest": Pipeline([
        ("preprocessor", preprocessor),
        ("model", RandomForestRegressor(random_state=42))
    ]),
    "XGBoost": Pipeline([
        ("preprocessor", preprocessor),
        ("model", XGBRegressor(eval_metric="rmse", random_state=42))
    ])
}

"""#### Hyperparameter Tuning

Tuning dilakukan untuk mengoptimalkan parameter secara efisien dan menghindari overfitting. Seluruh model dituning menggunakan RandomizedSearchCV untuk meningkatkan performa prediksi dan dibandingkan dengan hasil baseline.
"""

# Dictionary model hasil tuning
tuned_models = {}

# 1. Ridge Regression (tuning alpha = regularisasi)
ridge = Ridge()
param_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}
rs_ridge = RandomizedSearchCV(ridge, param_ridge, n_iter=5, scoring='neg_root_mean_squared_error', cv=5, random_state=42)
rs_ridge.fit(preprocessor.fit_transform(X), y)
tuned_models["Ridge Regression"] = Pipeline([("preprocess", preprocessor), ("model", rs_ridge.best_estimator_)])

# 2. Lasso Regression
lasso = Lasso()
param_lasso = {'alpha': [0.01, 0.1, 1, 10, 100]}
rs_lasso = RandomizedSearchCV(lasso, param_lasso, n_iter=5, scoring='neg_root_mean_squared_error', cv=5, random_state=42)
rs_lasso.fit(preprocessor.fit_transform(X), y)
tuned_models["Lasso Regression"] = Pipeline([("preprocess", preprocessor), ("model", rs_lasso.best_estimator_)])

# 3. Random Forest
rf = RandomForestRegressor(random_state=42)
param_rf = {
    'n_estimators': [100, 150, 200],
    'max_depth': [4, 6, 8, 10],
    'min_samples_split': [2, 5, 10]
}
rs_rf = RandomizedSearchCV(rf, param_rf, n_iter=5, scoring='neg_root_mean_squared_error', cv=5, random_state=42)
rs_rf.fit(preprocessor.fit_transform(X), y)
tuned_models["Random Forest"] = Pipeline([("preprocess", preprocessor), ("model", rs_rf.best_estimator_)])

# 4. XGBoost
xgb = XGBRegressor(eval_metric='rmse', random_state=42)
param_xgb = {
    'n_estimators': [100, 150, 200],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0]
}
rs_xgb = RandomizedSearchCV(xgb, param_xgb, n_iter=5, scoring='neg_root_mean_squared_error', cv=5, random_state=42)
rs_xgb.fit(preprocessor.fit_transform(X), y)
tuned_models["XGBoost"] = Pipeline([("preprocess", preprocessor), ("model", rs_xgb.best_estimator_)])

# Parameter terbaik dari RandomizedSearchCV
print("Ridge Regression:", rs_ridge.best_params_)
print("Lasso Regression:", rs_lasso.best_params_)
print("Random Forest   :", rs_rf.best_params_)
print("XGBoost         :", rs_xgb.best_params_)

"""## Model Evaluation

Evaluasi dilakukan menggunakan tiga metrik regresi utama, yaitu:

- **MAE (Mean Absolute Error)**: rata-rata selisih absolut antara nilai prediksi dan nilai asli.
- **RMSE (Root Mean Squared Error)**: mengukur besarnya error dengan penalti lebih besar untuk kesalahan besar.
- **R-squared Score**: proporsi variansi dalam target yang dapat dijelaskan oleh model.

Ketiga metrik ini digunakan untuk menilai sejauh mana prediksi model mendekati nilai asli.

#### Model with Default Parameter
"""

# Evaluasi hasil baseline
baseline_results = []

for name, model in baseline_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    baseline_results.append({
        "Model": name,
        "MAE": mean_absolute_error(y_test, y_pred),
        "RMSE": mean_squared_error(y_test, y_pred),
        "R-squared": r2_score(y_test, y_pred)
    })

# DataFrame hasil baseline
baseline_df = pd.DataFrame(baseline_results).sort_values(by="RMSE")
baseline_df

"""#### Model with Tuned Parameter"""

# Evaluasi hasil tuning model

tuning_results = []

for name, pipe in tuned_models.items():
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    tuning_results.append({
        "Model": name,
        "MAE": mae,
        "RMSE": rmse,
        "R-squared": r2
    })

# Buat DataFrame
results_df = pd.DataFrame(tuning_results).sort_values(by="RMSE")
results_df

"""#### Comparison of Baseline and Tuned Models"""

baseline_df['Status'] = 'Baseline'
results_df['Status'] = 'Tuned'

compare_df = pd.concat([baseline_df, results_df], ignore_index=True)

# MAE
plt.figure(figsize=(5, 3))
sns.boxplot(x='Status', y='MAE', data=compare_df)
plt.title('MAE: Baseline vs Tuned')
plt.tight_layout()
plt.show()

# RMSE
plt.figure(figsize=(5, 3))
sns.boxplot(x='Status', y='RMSE', data=compare_df)
plt.title('RMSE: Baseline vs Tuned')
plt.tight_layout()
plt.show()

# R-squared Score
plt.figure(figsize=(5, 3))
sns.boxplot(x='Status', y='R-squared', data=compare_df)
plt.title('R-squared: Baseline vs Tuned')
plt.tight_layout()
plt.show()

"""## Conclusion

Berdasarkan hasil evaluasi terhadap empat model regresi (Ridge, Lasso, Random Forest, dan XGBoost), tuning parameter menggunakan RandomizedSearchCV memberikan peningkatan performa secara umum dibandingkan model dengan parameter default (baseline).

- **Lasso Regression** menunjukkan peningkatan signifikan, dari RMSE 33.84 (baseline) menjadi 26.44 (tuned), dan R-squared meningkat dari 0.48 menjadi 0.60.
- **XGBoost dan Random Forest** juga mengalami penurunan nilai RMSE dan kenaikan R-squared, walaupun peningkatannya relatif kecil.
- **Ridge Regression** memiliki performa yang relatif stabil, dengan hasil tuning yang hampir sama dengan baseline.

Secara keseluruhan, tuning berhasil meningkatkan kinerja model, terutama pada model linear seperti Lasso, yang sebelumnya menunjukkan performa terburuk namun menjadi model terbaik setelah tuning. Dengan demikian, Proses tuning hyperparameter terbukti bermanfaat dalam meningkatkan akurasi prediksi model regresi untuk kasus ini.
"""